{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN0VqmFaUQJ2"
   },
   "source": [
    "##### *IMP-PCBA*\n",
    "\n",
    "# Self-study coding activity: Web crawling in action\n",
    "\n",
    "## Overview\n",
    "In this activity, you'll compare the use of Beautiful Soup and breadth-first search for web crawling. Use this notebook to follow along with Video 8.4.\n",
    "\n",
    "This activity is designed to build your familiarity and comfort coding in Python while also helping you review key topics from each module. As you progress through the activity, questions will get increasingly more complex. It is important that you adopt a programmer's mindset when completing this activity. Remember to run your code from each cell before submitting your activity, as doing so will give you a chance to fix any errors before submitting.\n",
    "\n",
    "### Learning outcome addressed\n",
    "- Compare the function of Beautiful Soup versus breadth-first search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, you learned about the Breadth First Search (BFS) algorithm and how it can be used for scraping web pages. The code below is the generic implementation of BFS:\n",
    "\n",
    "<code>\n",
    "    def get_children(node):\n",
    "    # this is the crux of the algo -- you'll need to implement it! \n",
    "        return \n",
    "\n",
    "    # the BFS algo\n",
    "    def breadth_first_search(root):\n",
    "        \"\"\"\n",
    "        Returns a list of all visited nodes\n",
    "        starting from a root node, or seed.\n",
    "        \"\"\"\n",
    "        visited = []\n",
    "        to_visit = [root]\n",
    "        while len(to_visit) > 0:\n",
    "            node = to_visit.pop(0) # poor man's implementation of a queue\n",
    "            if node not in visited:\n",
    "                to_visit.extend(get_children(node))\n",
    "                visited.append(node)\n",
    "        return visited\n",
    "</code>\n",
    "\n",
    "As mentioned in Video 8.4, the implementation of the **get_children** is very domain specific. For this exercise, the **get_children** refers to the hyperlinks contained within a string with html text.\n",
    "\n",
    "In the second part of this exercise, you will use BeautifulSoup, a Python library that makes it easier to parse and extract data from html pages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index:\n",
    "\n",
    "- [Question 1](#Question-1)\n",
    "- [Question 2](#Question-2)\n",
    "- [Question 3](#Question-3)\n",
    "- [Question 4](#Question-4)\n",
    "- [Question 5](#Question-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be working with the html from the [XKCD](https://xkcd.com/) comic strip. \n",
    "\n",
    "Run the cell below to initialize a variable called **xkcd_excerpt** containing some html text from the XKCD web site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBUTaXC3UQJ6"
   },
   "outputs": [],
   "source": [
    "xkcd_excerpt = \"\"\"<!DOCTYPE html>\n",
    "                <html>\n",
    "                <head>\n",
    "                <link rel=\"stylesheet\" type=\"text/css\" href=\"/s/b0dcca.css\" title=\"Default\"/>\n",
    "                <title>xkcd: Python</title>\n",
    "                <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"/>\n",
    "                <link rel=\"shortcut icon\" href=\"/s/919f27.ico\" type=\"image/x-icon\"/>\n",
    "                <link rel=\"icon\" href=\"/s/919f27.ico\" type=\"image/x-icon\"/>\n",
    "                <link rel=\"alternate\" type=\"application/atom+xml\" title=\"Atom 1.0\" href=\"/atom.xml\"/>\n",
    "                <link rel=\"alternate\" type=\"application/rss+xml\" title=\"RSS 2.0\" href=\"/rss.xml\"/>\n",
    "                <script type=\"text/javascript\" src=\"/s/b66ed7.js\" async></script>\n",
    "                <script type=\"text/javascript\" src=\"/s/1b9456.js\" async></script>\n",
    "\n",
    "                </head>\n",
    "                <body>\n",
    "                <div id=\"topContainer\">\n",
    "                <div id=\"topLeft\">\n",
    "                <ul>\n",
    "                <li><a href=\"/archive\">Archive</a></li>\n",
    "                <li><a href=\"http://what-if.xkcd.com\">What If?</a></li>\n",
    "                <li><a href=\"http://blag.xkcd.com\">Blag</a></li>\n",
    "                <li><a href=\"http://store.xkcd.com/\">Store</a></li>\n",
    "                <li><a rel=\"author\" href=\"/about\">About</a></li>\n",
    "                </ul>\n",
    "                </div>\n",
    "                <div id=\"topRight\">\n",
    "                <div id=\"masthead\">\n",
    "                <span><a href=\"/\"><img src=\"/s/0b7742.png\" alt=\"xkcd.com logo\" height=\"83\" width=\"185\"/></a></span>\n",
    "                <span id=\"slogan\">A webcomic of romance,<br/> sarcasm, math, and language.</span>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to top](#Index:) \n",
    "\n",
    "### Question 1\n",
    "\n",
    "Let's start by implementing a method called **get_next_url**.  This method takes a page (a string containing html text) and\n",
    "- Finds the index of the start of the **<a href=**  (start_link).\n",
    "- Finds the index of the first quote,  starting with the index of **start_link**  (start_quote).\n",
    "- Finds the index of the end quote, starting with the index of **start_quote** (end_quote).\n",
    "                                           \n",
    "Finally, the function returs the string that is between **start_quote** and **end_quote**.  It also returns the index of last quote (**end_quote**).\n",
    "                                                                                       \n",
    "**Hint**: Look up the documentation of Python string [find()](https://www.programiz.com/python-programming/methods/string/find).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_url(page):\n",
    "    start_link = None\n",
    "    start_quote = None\n",
    "    end_quote = None\n",
    "    url = page[None: None]\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your get_next_url function:\n",
    "get_next_url(xkcd_excerpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the get_next_url function\n",
    "\n",
    "Your function should return:\n",
    "\n",
    "<code>\n",
    "    ('/archive', 978)\n",
    "</code>\n",
    "\n",
    "As you can see, the first hyperlink the function finds in the text is:\n",
    "\n",
    "    <a href=\"/archive\">Archive</a>\n",
    "\n",
    "It returns the value contained within the double quotes (**/archive**) and the index of the last quote (978).  \n",
    "\n",
    "You will use the index of the last quote to decide where to start looking for your next hyperlink in the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to top](#Index:) \n",
    "\n",
    "### Question 2\n",
    "\n",
    "The next step is to define the function **get_all_urls**.  This function takes a string as a parameter containing an html text and returns a list of the hyperlinks found in the page.\n",
    "\n",
    "get_all_urls calls the function you just created called get_next_url.\n",
    "\n",
    "The idea is to loop inside function get_all_urls calling **get_next_url** until get_next_url does not find any more urls in the page.\n",
    "\n",
    "Also, note that you will use **end_quote** returned by the **get_next_url** function to slice the array containing the text. That way, you are always working with the next piece of the html text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls(page):\n",
    "    \"\"\"\n",
    "    this returns all the urls in the html source code \n",
    "    return a list with all the url strings\n",
    "    \"\"\"\n",
    "    url_list = [] # list()\n",
    "    while True:\n",
    "        url, end_quote = None(None)\n",
    "        if url:\n",
    "            url_list.append(url)\n",
    "            page = page[None:]\n",
    "        else:\n",
    "            break\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your get_all_urls function:\n",
    "get_all_urls(xkcd_excerpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the get_all_urls function\n",
    "\n",
    "Your function should return:\n",
    "\n",
    "<code>\n",
    "    ['/archive',\n",
    "     'http://what-if.xkcd.com',\n",
    "     'http://blag.xkcd.com',\n",
    "     'http://store.xkcd.com/',\n",
    "     '/']\n",
    "</code>\n",
    "\n",
    "Notice that inside the loop, the **page** string is reset to start where get_next_url last found a hyperlink (end_quote). The loop will stop when get_next_url does not find any more hyperlinks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to top](#Index:) \n",
    "\n",
    "### Question 3\n",
    "\n",
    "Now let's try to implement the BFS algorithm, and let's use a URL instead of a string.  Complete the definition of the function **get_children**.  It takes a URL containing a web page as a parameter and gets the html text for the web page as a string from the **requests** object. Then, it calls the function **get_all_urls**.\n",
    "\n",
    "The function **get_children** returns an array with all the hyperlinks contained in the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_children(url):\n",
    "    try:\n",
    "        page_source = None\n",
    "    except Exception:\n",
    "        page_source = ''\n",
    "    url_list = None\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's test the function:\n",
    "get_children('http://www.xkcd.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will return a list with the hyperlinks in the page.  The output should look like the following: \n",
    "<code>\n",
    "    \n",
    "['/archive',\n",
    " 'https://what-if.xkcd.com',\n",
    " '/atom.xml',\n",
    " '/newsletter/',\n",
    " 'https://twitter.com/xkcd/',\n",
    " 'https://www.facebook.com/TheXKCD/',\n",
    " 'https://www.instagram.com/xkcd/',\n",
    " '/books/',\n",
    " '/what-if-2/',\n",
    " '/what-if/',\n",
    " '/thing-explainer/',\n",
    " '/how-to/',\n",
    " '/',\n",
    " '/1/',\n",
    " '//c.xkcd.com/random/comic/',\n",
    " '/',\n",
    " '/1/',\n",
    " '//c.xkcd.com/random/comic/',\n",
    " '/',\n",
    " 'https://xkcd.com/2757',\n",
    " 'https://imgs.xkcd.com/comics/towed_message.png',\n",
    " '//xkcd.com/1732/',\n",
    " '/rss.xml',\n",
    " '/atom.xml',\n",
    " '/newsletter/',\n",
    " 'http://threewordphrase.com/',\n",
    " 'https://www.smbc-comics.com/',\n",
    " 'https://www.qwantz.com',\n",
    " 'https://oglaf.com/',\n",
    " 'https://www.asofterworld.com',\n",
    " 'https://buttersafe.com/',\n",
    " 'https://pbfcomics.com/',\n",
    " 'https://questionablecontent.net/',\n",
    " 'http://www.buttercupfestival.com/',\n",
    " 'https://www.homestuck.com/',\n",
    " 'https://www.jspowerhour.com/',\n",
    " 'https://medium.com/civic-tech-thoughts-from-joshdata/so-you-want-to-reform-democracy-7f3b1ef10597',\n",
    " 'https://www.nytimes.com/interactive/2017/climate/what-is-climate-change.html',\n",
    " 'https://twitter.com/KHayhoe',\n",
    " 'https://creativecommons.org/licenses/by-nc/2.5/']\n",
    " </code>\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to top](#Index:) \n",
    "\n",
    "### Question 4\n",
    "\n",
    "Now you are ready to put it all together and implement the BFS algorithm for the web crawler.\n",
    "\n",
    "Define a function called \"**crawl_web** that takes the start_url and the max_depth.\n",
    "\n",
    "The function returns a dictionary containing key/value pairs with the URL name and the list of hyperlinks for the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_web(start_url, max_depth):\n",
    "    \"\"\"\n",
    "    Returns a list of all visited nodes\n",
    "    starting from a root node, or seed.\n",
    "    \"\"\"\n",
    "    crawled = []\n",
    "    hyperlinksDict = {}\n",
    "    to_crawl = [[start_url]]\n",
    "    while to_crawl:\n",
    "        path = to_crawl.pop(0)\n",
    "        if len(path) > max_depth:\n",
    "            break\n",
    "        url = path[-1]\n",
    "        if url not in hyperlinksDict:\n",
    "            children = None\n",
    "            hyperlinksDict[None] = None \n",
    "            to_crawl.extend([path + [child] for child in children])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your crawl_web function - using max_depth=1\n",
    "xkcd_websites = crawl_web(start_url='http://www.xkcd.com', max_depth=1)\n",
    "xkcd_websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your crawl_web function - using max_depth=2\n",
    "xkcd_websites = crawl_web(start_url='http://www.xkcd.com', max_depth=2)\n",
    "xkcd_websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your crawl_web function - using max_depth=3\n",
    "xkcd_websites = crawl_web(start_url='http://www.xkcd.com', max_depth=3)\n",
    "xkcd_websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about testing it with a different url:\n",
    "start_url='https://www.imperial.ac.uk/people/f.nagle'\n",
    "websites = crawl_web(start_url=start_url, max_depth=2)\n",
    "websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you may overwhelm your machine if you use max_depth > 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to top](#Index:) \n",
    "\n",
    "### Question 5\n",
    "\n",
    "In this part of the exercise, you will be using BeautifulSoup to parse the html text.\n",
    "\n",
    "Use BeautifulSoup to parse the html text in **xkcd_excerpt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(xkcd_excerpt)\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to top](#Index:) \n",
    "\n",
    "### Question 6\n",
    "\n",
    "Using BeautifulSoup, find the **title** tag in the html text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr =  soup.find(None)\n",
    "print(hdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to top](#Index:) \n",
    "\n",
    "### Question 6\n",
    "\n",
    "Use [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/ ) to parse the data in **xkcd_excerpt** and extract the hyperlinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all(None, href=True):\n",
    "    print(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look like the following:\n",
    "<code>\n",
    "    /archive\n",
    "    http://what-if.xkcd.com\n",
    "    http://blag.xkcd.com\n",
    "    http://store.xkcd.com/\n",
    "    /about\n",
    "    /\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, BeatifulSoup makes it much easier to parse through html text and extract data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Back to top](#Index:) \n",
    "\n",
    "### Question 7\n",
    "\n",
    "Retrieve the web page from   [XKCD](https://xkcd.com/), and then use [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) to find all hyperlinks contained in the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmfbwKGEUQJ7",
    "outputId": "915ddcd9-1b9a-4d3c-8438-bb51dc76a1fc"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "page_source = requests.get(\"https://xkcd.com/\", timeout=1).text\n",
    "soup = BeautifulSoup(page_source)\n",
    "\n",
    "for link in soup.find_all(None, href=True):\n",
    "    print(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You completed this activity in which you implemented a BFS search algorithm for web crawling and also used BeaitufulSoup to parse and extract data from html text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
