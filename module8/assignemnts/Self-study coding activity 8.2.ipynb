{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1284199c-2fcc-449b-b105-7d13ca78abba",
   "metadata": {},
   "source": [
    "# Self-study coding activity 8.2: Web scraping in action "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c6fa3-5388-4a2c-86fc-42d934f87eb6",
   "metadata": {},
   "source": [
    "### 1. Setup and import libraries \n",
    "First, ensure you have the necessarry libraris installed. You need requests for fetching web pages and beautifulsoup4 for parding HTML content. Install them using pip if you haven't already: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb96986-44bb-4d2d-a148-752dacb266bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.13/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.13/site-packages (4.12.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.13/site-packages (from beautifulsoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8748f7de-1530-4aac-9814-31ac65744ccb",
   "metadata": {},
   "source": [
    "### 2. Send an HTTP request \n",
    "Use the requests library to send an HTTP request to the target website and fetch the HTML content of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbe4de9c-6d96-41c3-b400-eaf3c40af977",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://example.com'\n",
    "response = requests.get(url) \n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "   html_content = response.content\n",
    "else:\n",
    "    print(f'Failed to retrieve the page. Status code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad9664-4cc7-46d4-942b-c123ce5b35f0",
   "metadata": {},
   "source": [
    "### 3. Parse the HTML content\n",
    "Create a Beautiful Soup object to parse the HTML content retrieved from the website. This object allows you to navigate and search through the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b763749-1c42-4261-855e-5725c7ae484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37efb83-8cd3-49ef-ac7c-ce83a649bfe9",
   "metadata": {},
   "source": [
    "### 4. Navigate and search the HTML tree \n",
    "Use Beautiful Soup methods to find and extract the data you need. The methods you can use are listed below: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf579b78-0d75-4c65-bfb4-a86cd1f372bd",
   "metadata": {},
   "source": [
    "#### A. find():\n",
    "Finds the first occurrence of a tag or element in the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcb51047-5ffe-41b6-a4f4-546dd70e62ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to My Website\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "  <body>\n",
    "    <h1>Welcome to My Website</h1>\n",
    "    <p>This is the first paragraph.</p>\n",
    "    <h1>Another Heading</h1>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "first_h1 = soup.find('h1')\n",
    "print(first_h1.text)  # Output: Welcome to My Website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d492aa-6475-42ef-8afa-80a8fb721b45",
   "metadata": {},
   "source": [
    "#### B. find_all(): \n",
    "Finds all occurrences of a specific tag or element in the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ae2076a-b750-42e7-8d7b-6a4846940df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first paragraph.\n",
      "This is the second paragraph.\n",
      "This is the third paragraph.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "  <body>  \n",
    "    <p>This is the first paragraph.</p>  \n",
    "    <p>This is the second paragraph.</p>  \n",
    "    <p>This is the third paragraph.</p>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "all_paragraphs = soup.find_all('p')\n",
    "for p in all_paragraphs:    \n",
    "    print(p.text)\n",
    "# Output:\n",
    "# This is the first paragraph.\n",
    "# This is the second paragraph.\n",
    "# This is the third paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0709da-6d7d-4108-8f0f-1fa891cd469d",
   "metadata": {},
   "source": [
    "#### C. select: \n",
    "Finds elements using CSS selectors. This method is more flexible and allows you to use complex CSS-like rules. Suppose you want to find all elements with the class item and all tags within a specific with the ID content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28467caa-c698-486b-8b5a-f9432c457d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an item paragraph.\n",
      "This is another item paragraph.\n",
      "This is the first paragraph in the content.\n",
      "This is the second paragraph in the content.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "  <body>\n",
    "    <div id=\"content\">\n",
    "      <p>This is the first paragraph in the content.</p>\n",
    "      <p>This is the second paragraph in the content.</p>\n",
    "    </div>\n",
    "    <div class=\"item\">\n",
    "      <p>This is an item paragraph.</p>\n",
    "    </div>\n",
    "    <div class=\"item\">\n",
    "      <p>This is another item paragraph.</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all elements with the class \"item\"\n",
    "items = soup.select('.item')\n",
    "for item in items:\n",
    "    print(item.p.text)\n",
    "# Output:\n",
    "# This is an item paragraph.\n",
    "# This is another item paragraph.\n",
    "\n",
    "# Find all <p> tags within the div with id \"content\"\n",
    "content_paragraphs = soup.select('#content p')\n",
    "for p in content_paragraphs:\n",
    "    print(p.text)\n",
    "# Output:\n",
    "# This is the first paragraph in the content.\n",
    "# This is the second paragraph in the content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb1f98-fd30-4dad-8d1b-b2039f311dc3",
   "metadata": {},
   "source": [
    "### 5. Extract data\n",
    "The desired data can be extracted from the tags. If the h1 tags must be extracted from the URL, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a64a32dd-9881-40a6-a393-3146ef4c148b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Domain\n",
      "Example Domain\n",
      "https://iana.org/domains/example\n"
     ]
    }
   ],
   "source": [
    "url = 'http://example.com'\n",
    "response = requests.get(url)\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "  html_content = response.content\n",
    "else:\n",
    "    print(f'Failed to retrieve the page. Status code: {response.status_code}')\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "# Find the first <h1> tag\n",
    "h1_tag = soup.find('h1')\n",
    "if h1_tag:\n",
    "    print(h1_tag.text)\n",
    "else:\n",
    "    print('No <h1> tag found')\n",
    "# Extract text from an <h1> tag\n",
    "print(h1_tag.text)\n",
    "# Extract href attribute from all <a> tags\n",
    "a_tags = soup.find_all('a')\n",
    "for a in a_tags:\n",
    "  href = a.get('href')\n",
    "print(href)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a58c4-d305-4663-98a8-3b4827ecf44a",
   "metadata": {},
   "source": [
    "### 6. Handle relative URLs\n",
    "If you encounter relative URLs, convert them to absolute URLs to ensure they can be used to navigate the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb85cd4c-dc56-480c-8fb6-9a1d6e7b6586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.com/path/to/resource\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin\n",
    "base_url = 'http://example.com'\n",
    "relative_url = '/path/to/resource'\n",
    "absolute_url = urljoin(base_url, relative_url)\n",
    "print(absolute_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8eb8fd-3421-4c09-a51c-9076b9992cde",
   "metadata": {},
   "source": [
    "### 7. Store the Extracted Data \n",
    "The extracted data can be stored in any relevant format. It can be stored in text file, CSV file, JSON file, or directly into a database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec42a328-1b56-45bc-bb3d-c0fe5a0f73aa",
   "metadata": {},
   "source": [
    "#### A. Storing data in a text file: \n",
    "When storing as a text file, you can specify the path for storing or the default folder to be used for storing the file. Suppose you have an HTML page with several paragraphs, and you want to extract the text from each paragraph (<p> tag) and store it in a text file. The text file is the extracted file, and this file, by default, is stored in the same folder. However, you can store the file anywhere by defining the path of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f56ee6c-e5a7-4fdd-9abd-be36a5d58be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to extracted_data.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# For this example, we'll use the HTML content directly.\n",
    "# In a real scenario, you would fetch it from a website.\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "  <h1>Sample Page</h1>\n",
    "  <p>This is the first paragraph.</p>\n",
    "  <p>This is the second paragraph.</p>\n",
    "  <p>This is the third paragraph.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Parse the HTML content using Beautiful Soup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all <p> tags\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "# Open a text file in write mode\n",
    "with open('extracted_data.txt', 'w') as file:\n",
    "    # Loop through all <p> tags and write the text content to the file\n",
    "    for p in paragraphs:\n",
    "        file.write(p.get_text() + '\\n')\n",
    "\n",
    "print(\"Data has been written to extracted_data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f743f14-55ea-4820-b6db-8b8e9fada243",
   "metadata": {},
   "source": [
    "#### B. Storing data in CSV file: \n",
    "To store the output of webscraping using Beautiful Soup into a CSV file, perform the following steps:\n",
    "- Create an HTML string: Define the HTML content with a table. For example, a simple HTML document containing a table with three columns (Name,Age,City) is created.\n",
    "- Parse the HTML string: Use Beautiful Soup to parse the HTML content.\n",
    "- Extract the data: Extract the data from the table. The headers are extracted from the <th> elements. The rows are extracted from the <tr> elements, skipping the header row. Each cellâ€™s text content is extracted from the <td> elements.\n",
    "- Write to CSV: Export the extracted data to a CSV file. The csv.writer writes the headers and rows to a CSV file named output.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aed0cbf2-7b0e-4952-b08c-c7b9f47cc1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to output.csv\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Step 1: Create an HTML string with a table\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\">\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "  <title>Sample Table</title>\n",
    "</head>\n",
    "<body>\n",
    "  <table border=\"1\">\n",
    "      <tr>\n",
    "          <th>Name</th>\n",
    "          <th>Age</th>\n",
    "          <th>City</th>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>John Doe</td>\n",
    "          <td>30</td>\n",
    "          <td>New York</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>Jane Smith</td>\n",
    "          <td>25</td>\n",
    "          <td>Los Angeles</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>Emily Jones</td>\n",
    "          <td>35</td>\n",
    "          <td>Chicago</td>\n",
    "      </tr>\n",
    "  </table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Parse the HTML string\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Step 3: Extract the desired data\n",
    "data = []\n",
    "table = soup.find('table')  # Find the table\n",
    "if table:\n",
    "    headers = [header.text for header in table.find_all('th')]\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        data_row = [cell.text for cell in cells]\n",
    "        data.append(data_row)\n",
    "\n",
    "# Step 4: Write the data to a CSV file\n",
    "csv_file = 'output.csv'\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if headers:\n",
    "      writer.writerow(headers)\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Data has been written to {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b222b-1f58-4f72-88a4-350f9dde5fc2",
   "metadata": {},
   "source": [
    "#### 4. Storing data to a Database: \n",
    "To store scraped data in a database, perform the following steps:\n",
    "- Fetch the webpage: Use the requests library to fetch the content of the webpage. Use the HTML content by defining it or the URL for scraping data.\n",
    "- Parse the HTML: Use Beautiful Soup to parse the HTML content.\n",
    "- Extract the data: Identify and extract the specific data you need from the HTML. In this example, the script extracts headers and rows from the table.\n",
    "- Store in a database: Use a database library like sqlite3, SQLAlchemy, or any other database connector to store the extracted data.\n",
    "- Download the database: Once the db is created, it can be downloaded and opened in SQL lite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2c2b116-a182-4fc1-ad4e-8d01cf33b18c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 49)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<string>:49\u001b[0;36m\u001b[0m\n\u001b[0;31m    rows = table.find_all('tr')[1:]  # Skip the header row\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "\n",
    "# Step 1: Fetch the webpage (using a predefined HTML string for this example)\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\">\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "  <title>Sample Table</title>\n",
    "</head>\n",
    "<body>\n",
    "  <table border=\"1\">\n",
    "      <tr>\n",
    "          <th>Name</th>\n",
    "          <th>Age</th>\n",
    "          <th>City</th>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>John Doe</td>\n",
    "          <td>30</td>\n",
    "          <td>New York</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>Jane Smith</td>\n",
    "          <td>25</td>\n",
    "          <td>Los Angeles</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <td>Emily Jones</td>\n",
    "          <td>35</td>\n",
    "          <td>Chicago</td>\n",
    "      </tr>\n",
    "  </table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Parse the HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Step 3: Extract the desired data\n",
    "data = []\n",
    "table = soup.find('table')  # Find the table\n",
    "if table:\n",
    "    headers = [header.text for header in table.find_all('th')]\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "    for row in rows:\n",
    "      cells = row.find_all('td')\n",
    "        data_row = [cell.text for cell in cells]\n",
    "        data.append(data_row)\n",
    "\n",
    "# Step 4: Store the data in a database\n",
    "# Connect to SQLite database (or create it if it doesn't exist)\n",
    "conn = sqlite3.connect('example.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table\n",
    "cursor.execute('''\n",
    "  CREATE TABLE IF NOT EXISTS people (\n",
    "      id INTEGER PRIMARY KEY,\n",
    "      name TEXT,\n",
    "      age INTEGER,\n",
    "      city TEXT\n",
    "  )\n",
    "''')\n",
    "\n",
    "# Insert data into the table\n",
    "for row in data:\n",
    "  cursor.execute('INSERT INTO people (name, age, city) VALUES (?, ?, ?)', row)\n",
    "\n",
    "# Commit the transaction and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data has been written to the database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ca262-fc41-4be6-8124-f7b2847f7c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
